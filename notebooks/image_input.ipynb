{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://github.com/jerryjliu/llama_index/blob/main/examples/multimodal/Multimodal.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -Uqq llama-index langchain        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pinecone_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings(\n\u001b[1;32m     36\u001b[0m     openai_api_key\u001b[38;5;241m=\u001b[39mapi_key, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-embedding-ada-002\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m onecone_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPINECONE_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m pinecone\u001b[38;5;241m.\u001b[39minit(api_key\u001b[38;5;241m=\u001b[39m\u001b[43mpinecone_key\u001b[49m,environment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mus-west1-gcp-free\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m               )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Initialize the index\u001b[39;00m\n\u001b[1;32m     41\u001b[0m index \u001b[38;5;241m=\u001b[39m pinecone\u001b[38;5;241m.\u001b[39mIndex(index_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m295-youtube-index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pinecone_key' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import base64\n",
    "import requests\n",
    "import os \n",
    "import langchain\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt, retry_if_not_exception_type\n",
    "from tokenizers import Tokenizer\n",
    "import tiktoken\n",
    "# from tokenizers.models import BPE\n",
    "# from tokenizers.pre_tokenizers import Whitespace\n",
    "import openai\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import pinecone\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "# OpenAI API Key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(api_key)\n",
    "client = OpenAI(api_key=api_key)\n",
    "chat = ChatOpenAI(model=\"gpt-4-vision-preview\",\n",
    "                  max_tokens=4096, openai_api_key=api_key)\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=api_key, model=\"text-embedding-ada-002\")\n",
    "onecone_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone.init(api_key=pinecone_key,environment=\"us-west1-gcp-free\"\n",
    "              )\n",
    "# Initialize the index\n",
    "index = pinecone.Index(index_name=\"295-youtube-index\")\n",
    "\n",
    "# index.delete(delete_all=True, namespace=\"Slides\")\n",
    "\n",
    "# Get all the directories in the input directory\n",
    "directories = glob.glob(\n",
    "    \"/Users/ashriram/Documents/Website/website/cs295/assets/lectures/png/Part10/*/\")\n",
    "\n",
    "for directory_path in directories:\n",
    "    # Use glob to match the pattern '/*.png'\n",
    "    files = glob.glob(directory_path + \"*.png\")\n",
    "    # Get parent name\n",
    "    parent_directory_name = os.path.basename(os.path.dirname(os.path.dirname(directory_path)))\n",
    "    # print(parent_directory_name)    \n",
    "    # Get directory name \n",
    "    directory_name = os.path.basename(os.path.dirname(directory_path))\n",
    "    # print(directory_name)\n",
    "    path = os.path.join(parent_directory_name,directory_name) + \".pdf\"\n",
    "    metadatas = []\n",
    "    vectors = []\n",
    "    ids = []\n",
    "    print(path)\n",
    "    for image_path in tqdm(files):\n",
    "        # Get path to pdf \n",
    "        # Getting the base64 string\n",
    "        base64_image = encode_image(image_path)\n",
    "\n",
    "        content_json = chat.invoke(\n",
    "            [\n",
    "                HumanMessage(\n",
    "                    content=[\n",
    "                        {\"type\": \"text\",\n",
    "                            \"text\": \"Whatâ€™s in this image? Return the result as json with the following fields: title (string of 2-3 sentences), description (string))\"},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                                \"detail\": \"auto\",\n",
    "                            },\n",
    "                        },\n",
    "                    ]\n",
    "                )\n",
    "            ])\n",
    "        content_string = content_json.content.strip('```json\\n')\n",
    "        content_dict = json.loads(content_string)\n",
    "\n",
    "        content_dict[\"metadata\"] = {\"file\": \"\", \"slide\": \"\",\n",
    "                                    \"description\": \"#######CONTEXT#####\\n\" + content_dict[\"description\"]}\n",
    "        # Tokenize text and create embedding vector using openai\n",
    "        text = content_dict[\"title\"] + \" \" + content_dict[\"description\"]\n",
    "        query_result = embeddings.embed_query(text)\n",
    "        slide_number = os.path.splitext(os.path.basename(image_path))[0].replace(\"Slide\", \"\")\n",
    "        metadata = {\"file\": path , \"Slide\": slide_number, \"description\": text}\n",
    "        ids.append(path+\"#\"+slide_number)\n",
    "        metadatas.append(metadata)\n",
    "        vectors.append(query_result)\n",
    "    \n",
    "    index.upsert(zip(ids, vectors, metadatas), namespace=\"Slides\")\n",
    "\n",
    "        # Upsert data\n",
    "        # index.upsert([(path, query_result, metadata)], namespace=\"Slides\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to calculate cache size\n",
      "#######Slide Context#####\n",
      "The image displays a slide from a presentation addressing a computer science problem related to code analysis and cache memory. The slide is titled 'Example Code Analysis Problem' and presents a scenario where a cache memory starts cold with all blocks invalid. It includes a code snippet written in C that defines an 8x8 array and uses nested loops to sum its elements. The task is to calculate the cache miss rate given the cache size (C), block size (B), set-associativity (E), and address size (m).\n",
      "The image depicts a slide from a computer science lecture on cache parameters. It outlines a problem scenario involving cache memory configuration and asks students to fill in missing values in a table based on given specifications. The slide is titled 'Example Cache Parameters Problem' and includes instructions for a 4 KiB address space with 125 cycles to go to memory. A table with partially filled entries such as Cache Size (256 B), Block Size (32 B), Associativity (2-way), Hit Time (3 cycles), and Miss Rate (20%) is presented, with empty fields for Tag Bits, Index Bits, Offset Bits, and AMAT (Average Memory Access Time) to be calculated.\n",
      "Educational Slide on Cache Organization This image appears to be a slide from a computer science presentation, specifically addressing the topic of cache organization in computing. It discusses the cache size (C), which represents the amount of data a cache can store, and is given in bytes or number of blocks. The slide also mentions the necessity of a mechanism for mapping memory addresses to cache locations to allow fast access, suggesting the use of a hash table for quick data lookup. This is a typical slide one might find in a computer architecture or systems class.\n",
      "This image displays a slide from a computer science lecture about caches, specifically addressing concepts of cache size, block size, associativity, and address tag width. The slide is titled 'Peer Instruction Questions' and is labeled as part of a lecture series on caches (L15: Caches II). It contains two questions. The first question asks about the associativity of a cache that is 2 KiB in size with a block size of 128 B and has 2 sets, offering multiple-choice answers ranging from 2 to 'We're lost...'. The second question asks about the width of the Tag field for 16-bit wide addresses. The slide number at the bottom indicates it is the 21st slide in the series.\n",
      "The image depicts a slide from a computer science lecture, specifically focused on cache memory terminology and formulas. This slide is titled 'Notation Review' and emphasizes the importance of understanding various variable names related to cache memory. It includes a table with two columns, 'Variable' and 'This Quarter', followed by 'Formulas'. Variables such as block size (K), cache size (C), associativity (E), number of sets (S), address space (M), address width (m), tag field width (t), index field width (s), and offset field width (k) are listed with their corresponding notations and formulas to calculate their values.\n",
      "\n",
      " #######Audio Context#####\n",
      "How to calculate cache size So don't try to memorize this formula.  Try to work towards it based on your understanding of what these parameters represent.  But let's look at the size of the cache.  The size of the cache now is the number of the size of each block multiplied by the number of sets multiplied by the number of blocks per set, which is E.  The number of bits required for the set is given by first, let's calculate the number of sets in the cache.  The number of sets in the cache, capital S is equal to the number of blocks in the cache, which is the size of the cache divided by the size of A block in bytes.\n",
      " So to do that, we need to calculate how many sets or indices there are in my cache.  And remember the way we're going to calculate it is cache size divided by block size and the whole thing divided by the associativity factor E.  So cache size is 256.  Block size is 32.  And the associativity is 2.  So the number of sets capital S is equal to 4.\n",
      " The size of the cache now is the number of the size of each block multiplied by the number of sets multiplied by the number of blocks per set, which is E.  The number of bits required for the set is given by first, let's calculate the number of sets in the cache.  The number of sets in the cache, capital S is equal to the number of blocks in the cache, which is the size of the cache divided by the size of A block in bytes.  Divided by the associativity of the cache.  So that's going to give you the number of sets required by the cache.  The number of bits required to represent the number of sets is given by log to S.\n",
      " So a given cache, first of all, is going to be specified in terms of bytes.  So the cache size is specified as C.  So the more the cache size, the bigger the cache size, the more it's going to be effective  because it can store more bytes and supply it locally immediately to the processor's request.  The cache itself is split into some number of blocks.  So as an example, let's say you have a cache that's 32 kilobytes.\n",
      " So first, let's draw the memory.  So the size of the memory is 4, 0, 9, 6 bytes.  You've got your cache.  This is your cache.  The size of the cache is 256 bytes.  So capital C is 256.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```markdown\n",
       "### How to Calculate Cache Size\n",
       "\n",
       "To calculate the cache size (C) in bytes, follow the formula:\n",
       "\n",
       "C = (Block Size) * (Number of Sets) * (Associativity)\n",
       "\n",
       "Where:\n",
       "- Block Size (B) is the size of each cache block in bytes.\n",
       "- Number of Sets (S) is the total number of sets in the cache.\n",
       "- Associativity (E) is the number of blocks per set.\n",
       "\n",
       "#### Steps to Calculate:\n",
       "1. First, determine the Block Size (B), Associativity (E), and the total cache size (C) if not given.\n",
       "2. Calculate the Number of Sets (S) using the formula:\n",
       "   S = (C / B) / E\n",
       "3. The cache size (C) can then be found by rearranging the formula:\n",
       "   C = B * S * E\n",
       "\n",
       "#### Example:\n",
       "Given a cache with a total size of 256 bytes (C), a block size of 32 bytes (B), and an associativity of 2-way (E), the number of sets (S) can be calculated as:\n",
       "S = (256 bytes / 32 bytes) / 2\n",
       "S = 4 sets\n",
       "\n",
       "Thus, the cache size would be:\n",
       "C = 32 bytes * 4 sets * 2 (2-way set-associative)\n",
       "C = 256 bytes\n",
       "\n",
       "#### Notes:\n",
       "- The number of bits required to represent the number of sets is logâ‚‚(S).\n",
       "- Cache is generally specified in terms of bytes (C).\n",
       "- The larger the cache size, the more effective it is, as it can store more bytes and supply them locally to the processor's requests.\n",
       "- Remember that the cache is split into blocks, and the total cache size (C) is a product of the block size, number of sets, and associativity.\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "q_text = \"How to calculate cache size\"\n",
    "question = embeddings.embed_query(q_text)\n",
    "# Search for the most similar vector\n",
    "# print(results)\n",
    "context = q_text + \"\\n\" + \"#######Slide Context#####\\n\"\n",
    "results = index.query(queries=[question], top_k=5,\n",
    "                      namespace=\"Slides\", include_metadata=True)\n",
    "for r in results['results'][0]['matches']:\n",
    "    context += r['metadata']['description'] + \"\\n\"\n",
    "\n",
    "context += \"\\n #######Audio Context#####\\n\" + q_text \n",
    "\n",
    "results = index.query(queries=[question], top_k=5, include_metadata=True)\n",
    "\n",
    "for r in results['results'][0]['matches']:\n",
    "    context += r['metadata']['text'] + \"\\n\"\n",
    "    \n",
    "\n",
    "\n",
    "print(context)\n",
    "\n",
    "content_json = chat.invoke(\n",
    "        [\n",
    "            SystemMessage(content = \"You are an expert in RISC-V and Computer Architecture. Try to answer the following questions based on the context below in markdown format. If you don't know the answer, just say 'I don't know'.\"),\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\",\n",
    "                        \"text\": context}\n",
    "                ]\n",
    "            )\n",
    "        ])\n",
    "\n",
    "# Render as Markdown\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(content_json.content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
